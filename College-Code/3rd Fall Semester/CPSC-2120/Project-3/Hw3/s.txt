and
and
elementary
students
begin
their
studies
by
learning
the
fundamental
topics
of
and
the
study
of
students
begin
by
learning
to
comparable
elements
numbers
or
text
the
sorting
problem
asks
us
to
arrange
them
in
nondecreasing
is
a
truly
fundamental
algorithmic
has
been
the
focus
of
hundreds
of
publications
in
the
computer
science
and
it
is
almost
always
the
first
substantial
topic
covered
in
any
algorithms
amounts
of
computing
power
are
spent
solving
sorting
problems
in
and
sorting
is
also
a
key
preprocessing
step
or
subroutine
for
many
more
sophisticated
chapter
serves
not
only
as
an
introduction
to
sorting
but
also
as
an
introduction
to
fundamental
techniques
for
designing
algorithms
and
analyzing
their
correctness
and
running
is
the
ideal
domain
for
such
a
due
to
the
wide
variety
of
common
sorting
algorithms
out
also
study
two
problems
closely
related
to
the
largest
element
in
an
unsorted
and
topological
a
sequence
only
some
pairs
of
elements
can
be
meaningfully
assume
for
simplicity
that
our
input
comes
in
an
array
which
we
then
want
to
permute
so
that
sorting
algorithms
can
easily
be
adapted
to
work
directly
on
linked
lists
with
no
degradation
in
running
sorting
large
it
is
often
faster
in
practice
to
sort
an
array
of
pointers
to
these
since
this
involves
moving
less
memory
of
the
algorithms
in
this
chapter
are
designed
for
the
model
of
where
we
assume
that
input
elements
can
be
compared
and
nothing
our
input
elements
are
we
can
also
consider
using
the
or
real
real
turns
to
be
not
very
different
from
the
model
for
will
soon
show
that
there
is
an
lower
bound
for
sorting
in
both
of
these
so
sorting
algorithms
running
in
time
such
as
merge
sort
and
quicksort
are
optimal
in
this
the
since
we
can
assume
our
input
consists
of
we
will
be
able
to
design
algorithms
with
running
times
that
can
conceivably
run
as
long
as
they
are
sorting
integers
that
are
sufficiently
radix
only
takes
time
to
sort
integers
of
size
at
most
where
is
a
discussion
begins
with
three
simple
sorting
algorithms
that
illustrate
some
of
the
most
basic
techniques
of
algorithm
will
then
serve
as
examples
for
how
to
argue
both
the
correctness
and
running
time
of
an
involves
starting
with
an
arbitrary
solution
and
repeatedly
improving
it
with
small
modifications
until
it
eventually
becomes
prominent
algorithms
are
based
on
this
simple
including
a
wide
range
of
heuristics
for
obtaining
good
solutions
to
hard
problems
and
algorithms
for
solving
optimization
problems
the
heart
of
any
iterative
refinement
algorithm
is
usually
a
subroutine
that
either
proclaims
a
solution
to
be
or
identifies
some
aspect
of
the
solution
that
can
be
modified
to
make
it
more
if
we
scan
through
an
array
and
realize
it
is
not
then
there
must
be
some
adjacent
pair
of
elements
that
we
can
swap
to
make
the
array
will
formalize
this
notion
in
a
few
pages
when
we
talk
about
leads
to
the
bubble
which
repeatedly
scans
through
our
swapping
any
adjacent
pairs
of
elements
it
stopping
once
the
array
becomes
name
reflects
the
way
it
causes
small
elements
to
slowly
drift
toward
the
front
of
the
array
just
as
bubbles
drift
to
the
top
of
a
pool
of
of
a
single
iteration
of
insertion
where
is
inserted
within
the
sorted
prefix
and
a
snapshot
in
the
middle
of
the
process
of
merging
two
sorted
arrays
and
sorting
a
stack
of
you
may
instinctively
use
an
insertion
where
you
maintain
two
stacks
of
those
sorted
so
and
the
remaining
unsorted
every
you
insert
a
paper
from
the
unsorted
stack
into
its
proper
location
in
the
sorted
sort
is
an
example
of
an
algorithm
design
technique
called
incremental
where
a
solution
is
built
up
step
by
one
element
at
a
applying
insertion
sort
to
an
array
the
consists
of
some
prefix
whose
elements
are
in
sorted
expand
this
prefix
in
each
as
shown
in
by
moving
the
next
element
into
its
proper
location
so
as
to
leave
is
sometimes
done
by
taking
and
repeatedly
swapping
it
backwards
as
long
as
it
is
preceded
by
a
larger
takes
time
to
process
each
successive
for
a
total
running
time
of
can
also
think
of
incremental
algorithms
from
a
rather
than
point
of
we
could
say
that
insertion
sort
first
recursively
sorts
then
inserts
the
final
element
into
its
proper
location
so
the
whole
array
becomes
difference
is
only
in
how
we
choose
to
think
about
the
both
variants
perform
essentially
the
same
operations
and
have
identical
running
and
algorithms
construct
a
large
solution
out
of
solutions
to
smaller
instances
of
the
same
incremental
we
successively
add
one
element
at
a
taking
a
solution
to
a
subproblem
of
size
and
somehow
augmenting
it
with
one
additional
way
of
divide
and
algorithms
tend
to
decompose
a
larger
problem
in
a
more
fashion
for
splitting
a
problem
of
size
into
subproblems
of
size
popular
sorting
algorithm
based
on
divide
and
conquer
is
merge
based
on
the
fundamental
process
of
two
sorted
sequences
into
one
larger
sorted
is
quite
straightforward
to
merge
two
sorted
stacks
of
you
can
repeatedly
compare
the
front
pages
of
both
always
selecting
the
smaller
for
the
next
page
in
the
merged
shown
in
we
merge
two
arrays
and
the
same
maintain
pointers
and
to
the
elements
of
both
and
every
iteration
we
select
the
smaller
of
these
to
be
the
next
element
in
the
merged
advancing
its
corresponding
from
a
recursive
point
of
we
select
the
smaller
of
and
to
be
first
in
the
merged
and
the
rest
of
the
merged
array
is
formed
by
recursively
merging
the
leftover
parts
of
and
takes
time
to
merge
two
arrays
of
combined
length
since
each
step
places
one
element
into
its
correct
final
position
in
the
merged
merge
sort
an
array
we
recursively
merge
sort
the
first
half
and
second
half
then
merge
the
results
you
prefer
an
iterative
the
same
process
can
be
described
as
in
regard
an
array
as
adjacent
sorted
then
merge
these
pairwise
to
obtain
adjacent
sorted
then
adjacent
sorted
and
so
processes
do
the
same
and
differ
only
in
whether
we
prefer
a
recursive
perspective
the
more
natural
way
to
describe
a
divide
and
conquer
algorithm
like
merge
or
a
iterative
sort
runs
in
as
we
shall
see
in
a
time
of
merge
sort
computed
three
by
noting
that
we
spend
time
per
by
solving
the
recurrence
and
via
an
iterative
perspective
where
the
algorithm
runs
in
each
taking
and
it
may
seem
obvious
that
the
three
sorting
algorithms
above
are
caution
is
advised
algorithms
that
seem
correct
at
first
glance
may
often
fail
for
subtle
and
only
with
a
clear
mathematical
analysis
can
one
establish
correctness
beyond
any
reasonable
can
argue
correctness
at
many
levels
of
this
we
tend
to
stay
at
a
high
although
for
applications
one
can
also
argue
correctness
of
an
algorithm
or
computer
program
in
excruciating
this
we
discuss
techniques
for
analyzing
correctness
and
running
is
the
most
important
of
the
of
since
there
is
still
value
in
a
correct
algorithm
whose
running
time
we
do
not
fully
but
a
fast
incorrect
algorithm
is
typically
of
little
that
an
algorithm
eventually
terminates
is
also
a
key
part
of
proving
iterative
refinement
termination
usually
implies
that
the
algorithm
has
reached
a
correct
so
the
entire
correctness
argument
reduces
to
proving
on
correctness
arguments
are
structured
as
proofs
by
is
particularly
well
suited
for
algorithms
that
build
large
solutions
out
of
smaller
since
it
allows
us
to
assume
by
induction
on
problem
size
that
our
algorithm
will
correctly
solve
any
subproblem
of
strictly
smaller
merge
sort
for
sort
correctly
sorts
any
array
of
length
is
easily
proved
using
induction
on
a
base
case
key
part
of
any
inductive
correctness
is
trivially
verified
for
induction
tells
us
that
our
recursive
calls
to
merge
sort
will
properly
sort
the
first
and
second
halves
of
the
that
remains
is
to
argue
correctness
of
our
merging
which
we
also
accomplish
using
how
we
apply
induction
in
this
depends
on
whether
we
are
phrasing
the
merge
as
an
iterative
algorithm
or
as
a
recursive
that
a
simple
recursive
way
to
merge
two
sorted
lists
and
is
to
take
the
smaller
of
their
initial
elements
as
the
first
element
in
the
merged
and
then
to
complete
the
output
by
recursively
merging
the
contents
of
and
correctness
easily
follows
from
induction
on
the
combined
length
of
and
the
first
element
we
choose
is
clearly
the
smallest
and
hence
the
correct
element
to
place
first
in
the
merged
then
claim
by
induction
that
the
remainder
of
and
will
be
merged
since
this
constitutes
a
smaller
problem
and
proofs
are
also
often
applied
to
iterative
we
usually
apply
induction
on
the
number
of
iterations
of
our
and
our
inductive
hypothesis
is
called
a
loop
is
a
property
that
always
holds
at
key
points
during
an
and
a
loop
invariant
is
a
condition
that
remains
true
at
the
beginning
of
every
iteration
of
a
a
simple
consider
randomly
permuting
the
contents
of
an
array
as
be
a
random
number
in
suitable
loop
invariant
here
is
that
the
elements
in
are
equally
likely
to
appear
in
any
of
their
possible
is
true
as
a
base
case
when
the
loop
and
one
can
show
that
it
is
maintained
by
each
iteration
of
the
loop
when
reaches
the
invariant
implies
that
we
have
indeed
randomly
permuted
of
an
appropriate
loop
please
give
a
short
proof
of
correctness
for
the
iterative
approach
for
merging
shown
in
of
up
the
are
several
methods
we
can
use
to
determine
the
total
running
time
of
an
depending
on
its
a
recursive
we
usually
compute
running
time
by
solving
a
if
denotes
the
running
time
of
merge
sort
on
we
know
that
since
we
are
performing
two
recursive
sorts
each
on
followed
by
a
merge
solution
of
this
recurrence
gives
the
running
time
of
merge
iterative
simple
loop
counting
often
shown
in
we
can
think
of
merge
sort
as
an
iterative
algorithm
that
takes
singleton
elements
and
merges
them
pairwise
to
obtain
sorted
lists
of
length
then
merges
these
pairwise
to
obtain
sorted
lists
of
length
and
so
on
for
such
merging
is
a
each
phase
sorted
lists
of
length
takes
for
a
total
of
from
that
solving
a
recurrence
involves
adding
up
the
work
done
by
an
algorithm
at
each
of
you
look
at
you
will
also
see
a
tree
of
recursive
subproblems
whose
work
we
have
added
up
level
by
in
this
the
analysis
of
our
iterative
outlook
on
merge
sort
turns
out
to
be
just
another
way
of
doing
the
same
math
we
used
to
solve
the
merge
sort
per
of
adding
up
the
total
time
spent
in
each
step
of
an
summed
over
all
steps
during
the
it
is
sometimes
more
convenient
to
consider
the
total
running
time
spent
on
each
individual
input
summed
over
all
adds
up
the
same
amount
of
total
just
in
a
more
convenient
merge
sort
as
our
example
again
it
takes
time
to
merge
two
sorted
arrays
of
combined
length
which
is
time
per
element
taking
part
in
the
total
amount
of
work
merge
sort
spends
on
a
single
element
of
data
is
therefore
proportional
to
the
number
of
merge
operations
in
which
the
element
takes
many
merges
happen
to
a
single
you
put
yourself
in
the
perspective
of
an
element
of
you
will
find
that
every
time
you
take
part
in
a
you
end
up
in
a
sorted
subarray
twice
as
large
as
will
happen
times
before
you
end
up
in
a
sorted
array
of
length
therefore
spend
time
per
for
a
total
running
time
of
sort
of
outlook
on
running
time
can
be
highly
useful
in
the
analysis
of
many
and
common
and
powerful
technique
for
proving
correctness
and
running
time
is
the
use
of
a
potential
potential
function
maps
the
state
of
our
algorithm
to
a
nonnegative
we
can
guarantee
that
this
must
decrease
during
each
iteration
of
the
then
termination
is
approach
is
commonly
used
with
algorithms
based
on
iterative
where
our
potential
function
usually
reflects
the
amount
of
inherent
in
the
current
is
a
natural
physical
analogy
suggested
by
the
use
of
the
term
we
can
think
of
a
potential
function
as
telling
us
the
amount
of
stored
in
our
current
with
our
algorithm
acting
in
the
role
of
pulling
in
a
direction
that
decreases
this
potential
a
natural
potential
function
is
the
number
of
in
our
pair
of
elements
and
with
constitutes
an
inversion
if
that
the
elements
are
ordered
incorrectly
with
respect
to
their
sorted
array
has
no
and
a
array
has
the
maximum
possible
number
of
since
every
pair
is
an
show
up
often
in
the
study
of
permutations
and
and
since
inversion
count
is
a
natural
way
to
measure
it
is
often
a
good
choice
for
a
potential
swapping
two
adjacent
elements
we
correct
a
single
leaving
all
others
this
tells
us
that
bubble
sort
must
since
each
scan
through
the
array
the
performs
at
least
one
such
decreasing
the
inversion
functions
are
often
useful
for
analyzing
running
denote
the
sorted
version
of
our
array
and
suppose
our
potential
function
tells
us
the
size
of
the
largest
suffix
of
our
array
such
that
the
number
of
elements
at
the
end
of
that
are
in
their
correct
final
scan
of
bubble
sort
increases
this
potential
by
at
least
first
scan
pulls
the
largest
through
repeated
to
its
correct
final
position
at
the
end
of
the
then
the
second
scan
pulls
the
element
back
to
the
and
so
our
potential
cannot
exceed
we
perform
at
most
each
taking
for
a
total
running
time
of
inversion
count
as
a
potential
insertion
sort
has
a
fixed
overhead
for
scanning
the
array
and
then
corrects
one
inversion
for
each
backward
swap
it
the
running
time
is
where
is
the
number
of
inversions
in
our
input
is
still
in
the
worst
but
can
potentially
be
much
faster
on
in
problem
we
will
design
an
variant
of
insertion
sort
whose
running
time
scales
gracefully
as
a
function
of
between
for
a
sorted
array
no
up
to
for
a
array
the
maximum
possible
number
of
where
it
can
even
outperform
merge
if
then
insertion
sort
runs
in
while
merge
sort
runs
in
time
that
merge
sort
always
runs
in
even
when
given
an
we
study
amortized
analysis
in
the
next
we
will
make
somewhat
more
sophisticated
running
time
arguments
by
using
potential
functions
that
can
both
increase
and
rather
than
moving
in
a
single
monotonic
and
we
wish
to
design
a
sorting
algorithm
in
which
the
only
operation
available
is
a
cyclic
shift
of
consecutive
a
left
cyclic
shift
on
the
middle
characters
of
the
string
yields
inversions
and
please
characterize
the
inputs
for
which
sorting
is
possible
in
this
and
show
how
to
sort
them
in
insertion
and
merge
sort
are
all
comparison
model
is
ideal
for
allowing
us
to
develop
algorithms
that
can
sort
any
type
of
comparable
data
real
text
this
generality
has
a
as
there
is
an
lower
bound
on
the
running
time
of
any
algorithm
for
sorting
algorithm
for
sorting
a
array
modeled
as
a
decision
are
assumed
to
be
distinct
for
otherwise
each
comparison
would
generate
three
possible
or
by
can
easily
show
an
upper
bound
of
on
the
running
time
required
to
sort
by
demonstrating
an
like
merge
a
lower
bound
is
since
this
involves
proving
that
sorting
no
matter
how
bizarre
or
must
take
steps
in
the
worst
make
such
a
we
first
show
how
any
sorting
algorithm
can
be
abstractly
represented
using
a
simple
algorithmic
model
called
a
decision
then
we
show
that
any
decision
tree
must
make
comparisons
in
the
worst
case
in
order
to
properly
sort
sorting
algorithm
in
the
form
of
a
decision
tree
is
shown
in
every
it
compares
two
elements
and
branches
based
on
the
more
comparisons
the
algorithm
the
more
it
learns
about
the
ordering
of
the
input
we
reach
a
the
algorithm
terminates
and
declares
how
to
rearrange
the
input
in
sorted
height
of
the
tree
is
the
number
of
comparisons
required
in
the
worst
sorting
algorithm
can
be
abstractly
represented
this
as
a
series
of
decision
one
for
each
input
size
we
are
sorting
distinct
input
there
are
different
orderings
we
might
receive
as
one
needs
to
be
differently
in
order
to
sort
them
all
decision
tree
therefore
must
have
at
least
two
different
input
orderings
would
find
their
way
to
the
same
two
orderings
would
the
to
our
generating
identical
comparison
results
along
their
path
down
the
the
same
permutation
would
be
applied
to
one
of
them
would
therefore
be
sorted
decision
tree
of
height
can
have
at
most
and
we
need
so
by
height
of
our
decision
tree
the
number
of
comparisons
needed
to
sort
elements
in
the
worst
must
therefore
be
result
of
a
comparison
is
one
of
three
or
any
sorting
algorithm
must
still
properly
sort
in
the
special
case
where
we
have
distinct
it
suffices
to
consider
only
the
cases
and
for
the
argument
plays
a
more
central
role
in
several
other
the
element
uniqueness
asks
if
they
are
all
distinct
or
if
there
exist
two
equal
two
sets
and
each
with
at
most
the
set
disjointness
asks
whether
and
share
any
elements
in
common
we
want
to
test
if
two
sets
and
with
the
set
equality
involves
testing
whether
each
of
we
can
use
decision
trees
to
argue
an
lower
bound
in
the
comparison
although
the
proofs
are
slightly
more
nuanced
than
the
one
above
for
sorting
we
will
see
in
how
to
use
to
solve
all
three
problems
in
expected
time
in
the
illustrating
well
the
importance
of
our
underlying
computational
for
up
to
out
of
individuals
have
a
particular
disease
with
much
smaller
than
would
like
to
identify
them
with
a
minimum
number
of
expensive
you
could
test
all
individuals
but
it
can
be
much
less
costly
to
test
groups
of
we
group
blood
samples
from
a
set
of
the
outcome
of
a
single
test
tells
us
whether
at
least
one
individual
in
has
the
or
nobody
in
has
the
may
have
seen
problems
of
this
sort
in
recreational
mathematics
involving
the
identification
of
coins
of
incorrect
weight
by
weighing
groups
of
coins
on
a
solution
can
be
in
that
results
from
one
aggregate
test
can
influence
your
choice
of
which
group
to
test
next
consider
the
more
challenging
case
later
in
problem
show
a
simple
algorithm
that
identifies
all
of
the
sick
individuals
with
and
use
a
decision
tree
argument
to
show
that
tests
are
necessary
in
the
worst
and
extend
our
lower
bound
to
sorting
let
us
think
of
a
randomized
algorithm
abstractly
as
just
a
probability
distribution
over
deterministic
imagine
taking
all
the
random
bits
our
algorithm
will
consume
during
its
and
using
these
up
front
to
select
a
deterministic
algorithm
from
a
huge
this
initial
the
algorithm
is
purely
since
all
of
its
behavior
has
been
fully
to
this
we
can
model
a
randomized
sorting
algorithm
as
a
probability
distribution
over
decision
each
of
these
trees
must
still
sort
properly
it
must
have
our
bound
on
comparisons
still
we
rarely
apply
analysis
to
a
randomized
it
is
far
more
typical
to
look
at
running
in
we
will
see
how
to
use
techniques
from
game
theory
to
prove
lower
bounds
on
the
expected
performance
of
randomized
and
we
will
establish
an
lower
bound
on
the
expected
running
time
of
any
randomized
sorting
bound
is
closely
related
to
the
result
that
any
deterministic
algorithm
must
take
expected
time
to
sort
elements
that
have
been
permuted
at
this
is
by
modifying
the
proof
above
to
show
that
a
decision
tree
with
leaves
must
have
leaf
becomes
more
challenging
to
argue
lower
bounds
when
we
move
away
from
the
comparison
model
towards
a
more
general
model
like
the
real
where
input
data
is
now
if
we
consider
the
element
uniqueness
problem
defined
above
on
a
set
of
real
numbers
we
could
conceivably
solve
this
problem
by
evaluating
the
product
of
over
all
pairs
and
making
only
a
single
comparison
to
test
if
this
product
is
equal
to
it
may
seem
unlikely
that
this
approach
could
possibly
yield
an
efficient
even
though
it
seems
to
involve
this
product
can
be
evaluated
in
only
time
using
the
as
we
shall
see
in
the
point
is
that
it
no
longer
suffices
to
consider
only
comparisons
in
order
to
obtain
a
meaningful
lower
help
us
obtain
lower
bounds
in
the
real
we
turn
to
a
generalization
of
the
decision
tree
known
as
the
algebraic
computation
which
models
any
real
algorithm
by
a
tree
containing
both
branching
nodes
at
which
we
perform
and
nodes
at
which
we
perform
elementary
operations
such
as
discussion
of
this
model
is
beyond
the
scope
of
this
one
can
show
that
as
well
as
the
other
three
example
problems
above
set
and
set
all
have
lower
bounds
in
the
real
model
through
the
use
of
algebraic
computation
via
a
from
problem
to
problem
is
an
important
concept
in
algorithmic
since
it
allows
us
to
transfer
hardness
results
from
problem
to
problem
in
we
showed
how
problem
must
be
if
it
can
be
reduced
in
polynomial
time
to
another
problem
that
is
known
to
be
that
we
have
established
lower
bounds
on
running
time
for
a
handful
of
problems
like
sorting
and
element
we
can
similarly
use
reductions
to
transfer
these
bounds
to
other
an
let
problem
be
element
with
an
worst
case
lower
bound
in
the
comparison
and
real
will
transfer
this
lower
bound
via
straightforward
reduction
to
problem
which
asks
us
to
permute
an
array
so
equal
elements
are
grouped
together
in
contiguous
blocks
necessarily
appearing
in
sorted
that
we
can
solve
element
uniqueness
by
first
grouping
equal
elements
then
scanning
in
time
to
check
if
all
neighboring
elements
are
if
problem
could
be
solved
in
faster
than
the
same
would
be
true
for
element
contradicting
the
lower
bound
on
element
uniqueness
in
the
comparison
and
real
an
array
about
a
pivot
element
that
although
have
drawn
the
output
partitioning
is
usually
done
in
the
same
memory
space
as
the
original
use
reductions
from
sorting
or
other
problems
above
set
set
to
show
bounds
for
the
following
the
following
problems
in
the
comparison
counting
the
number
of
occurrences
of
a
element
in
an
and
given
two
sets
and
with
compute
or
it
not
possible
to
build
a
data
structure
supporting
the
operations
and
all
in
problems
in
computational
geometry
inherit
lower
bounds
from
element
or
their
the
following
problems
in
the
real
since
they
involve
points
with
numeric
the
closest
problem
asks
us
to
find
the
closest
pair
of
points
within
a
set
of
points
in
the
and
the
congruence
problem
gives
us
two
sets
and
in
the
and
asks
whether
can
be
obtained
from
by
applying
a
rigid
transformation
a
translation
plus
a
rotation
plus
a
of
the
most
popular
and
sorting
algorithms
is
which
like
merge
sort
is
a
based
on
the
principle
of
divide
and
quicksort
is
almost
symmetric
to
merge
sort
in
its
merge
sort
performs
two
recursive
sorts
followed
by
a
whereas
quicksort
performs
a
followed
by
two
recursive
is
shown
in
a
specific
array
element
as
a
we
rearrange
the
array
into
a
block
of
elements
less
than
or
equal
to
the
followed
by
the
pivot
followed
by
a
block
of
elements
greater
than
or
equal
to
the
pivot
element
will
then
be
in
its
correct
final
and
quicksort
finishes
by
recursively
sorting
the
subarrays
left
and
right
of
the
is
very
straightforward
if
we
allocate
a
temporary
buffer
the
same
size
as
our
array
to
hold
the
this
we
simply
scan
through
our
array
and
copy
out
all
the
elements
less
than
the
pivot
to
the
beginning
of
the
and
all
those
greater
than
the
pivot
to
the
end
of
the
partitioning
is
more
commonly
performed
without
the
need
for
an
auxiliary
common
approaches
for
doing
this
are
explained
in
methods
for
partitioning
an
array
in
we
scan
pointers
and
inward
from
the
ends
of
the
stopping
when
is
larger
than
the
pivot
and
is
smaller
than
the
then
swap
and
and
continue
scanning
until
the
pointers
in
we
scan
two
pointers
from
left
to
each
if
is
larger
than
the
we
simply
advance
we
swap
and
and
advance
both
and
a
tricky
aspect
of
quicksort
is
how
to
choose
the
we
should
choose
the
median
which
we
will
shortly
learn
how
to
find
in
only
the
median
evenly
splits
our
array
into
two
recursive
subproblems
of
size
the
running
time
of
quicksort
is
then
described
by
the
recurrence
with
the
term
representing
both
the
time
spent
finding
the
median
and
the
time
spent
this
solves
to
the
running
time
required
to
find
the
median
element
has
a
rather
high
hidden
so
this
version
of
quicksort
is
likely
to
run
significantly
slower
than
merge
sort
in
even
though
the
two
share
the
same
asymptotic
running
partition
need
to
be
perfectly
if
we
could
with
a
running
time
of
albeit
with
a
much
higher
hidden
constant
the
solution
of
is
still
the
other
if
we
always
pick
a
very
bad
pivot
like
the
minimum
or
maximum
the
running
time
can
be
as
slow
as
since
each
partition
operation
makes
very
little
leaving
us
with
a
subproblem
of
size
simple
but
flawed
strategy
for
selecting
a
pivot
element
is
just
to
pick
whichever
element
happens
to
be
at
the
beginning
or
end
of
the
if
our
array
is
already
sorted
common
occurrence
in
this
is
a
very
bad
slightly
more
popular
heuristic
is
to
look
at
the
elements
that
appear
and
in
the
middle
of
the
array
and
to
use
the
median
of
these
three
as
a
of
approach
can
still
lead
to
a
running
time
of
on
contrived
but
in
practice
it
has
been
observed
to
perform
reasonably
we
pick
the
pivot
element
at
we
can
use
a
simple
deterministic
pivoting
strategy
such
as
choosing
the
first
element
in
the
after
randomly
permuting
the
array
as
a
preprocessing
an
approach
that
seems
intuitively
sensible
since
it
should
generate
reasonable
partitions
most
of
the
this
known
as
randomized
runs
in
time
both
in
expectation
and
with
high
quicksort
is
simple
to
implement
and
competitive
with
merge
sort
in
show
that
the
expected
running
time
of
randomized
quicksort
is
we
use
linearity
of
expectation
to
decompose
its
running
time
into
a
sum
of
much
simpler
random
are
several
nice
ways
to
do
can
add
up
the
total
number
of
comparisons
performed
by
the
algorithm
is
asymptotically
the
same
as
its
running
by
defining
an
indicator
random
variable
for
each
pair
of
elements
taking
the
value
if
they
are
and
can
add
up
the
total
expected
work
spent
on
subproblems
of
different
we
take
the
over
all
of
the
expected
work
spent
partitioning
all
subproblems
of
size
seen
during
the
expected
number
of
which
is
surprisingly
easy
to
can
show
that
expected
time
is
spent
on
each
individual
element
in
our
using
the
randomized
reduction
a
high
probability
we
observe
from
the
last
approach
that
the
randomized
reduction
lemma
also
tells
us
that
we
spend
time
on
each
element
with
high
taking
a
union
bound
over
all
the
total
running
time
is
therefore
also
with
high
and
are
given
nuts
of
different
sizes
and
corresponding
and
we
must
match
these
determining
for
each
nut
the
unique
bolt
of
the
same
it
is
difficult
to
compare
two
nuts
or
two
bolts
it
is
only
possible
to
compare
a
nut
against
a
bolt
to
see
if
the
nut
fits
the
if
it
is
too
or
if
it
is
too
a
randomized
algorithm
running
in
time
with
high
probability
that
will
properly
match
the
nuts
and
spends
a
lot
of
time
on
recursive
function
call
overhead
for
very
small
a
common
trick
to
improve
performance
in
practice
is
to
of
its
recursion
earlier
by
leaving
sufficiently
small
arrays
with
at
most
gives
a
final
array
that
is
nearly
to
which
we
apply
insertion
sort
as
a
postprocessing
and
we
will
learn
about
a
powerful
and
versatile
data
structure
called
a
binary
search
which
essentially
encodes
the
recursive
tree
of
all
subproblems
generated
by
saving
this
we
can
the
sorting
quickly
computing
changes
to
a
sorted
ordering
in
response
to
insertion
or
deletion
of
fundamental
algorithms
related
to
such
as
binary
and
quickselect
are
elegantly
expressed
in
the
context
of
binary
search
we
postpone
further
discussion
until
advantage
of
quicksort
over
merge
sort
is
the
fact
that
quicksort
can
run
in
it
rearranges
the
contents
of
the
input
array
using
only
the
memory
space
allocated
for
the
and
almost
no
auxiliary
merging
elements
seems
to
require
that
we
allocate
a
temporary
array
of
size
to
hold
the
output
of
the
modern
times
when
computer
memory
is
relatively
there
tends
to
be
less
emphasis
on
the
importance
of
space
versus
running
time
rather
than
space
is
typically
the
limiting
resource
for
most
algorithms
in
for
the
case
of
memory
usage
tends
to
be
studied
a
bit
more
since
many
applications
involve
sorting
very
large
data
strictest
definition
of
an
algorithm
would
allow
extra
words
of
memory
in
addition
to
the
memory
occupied
by
the
this
can
make
the
implementation
of
recursive
algorithms
since
there
is
often
not
enough
stack
space
to
store
the
state
of
unfinished
recursive
we
typically
call
an
algorithm
if
it
uses
only
extra
words
of
earlier
analysis
of
randomized
quicksort
showed
that
its
stack
depth
is
with
high
although
the
worst
case
is
still
if
we
have
phenomenally
bad
luck
in
our
selection
of
pivot
we
want
to
ensure
stack
depth
in
the
worst
case
irrespective
of
pivot
a
clever
trick
is
to
have
quicksort
always
recurse
on
the
smaller
of
its
two
subproblems
each
such
recursive
call
halves
the
size
of
our
current
this
limits
recursion
depth
to
second
recursive
call
in
quicksort
is
what
is
known
as
tail
it
is
the
last
thing
done
by
the
quicksort
there
is
no
need
for
additional
stack
space
to
enact
this
since
we
need
to
save
the
state
of
the
current
function
simply
the
same
stack
frame
from
the
current
effectively
jumping
right
back
to
the
start
of
the
quicksort
function
with
new
merge
sort
does
not
operate
in
since
it
merges
two
arrays
of
combined
length
into
a
memory
buffer
of
length
this
your
goal
is
to
develop
an
sorting
algorithm
based
fundamentally
on
merging
is
of
course
easy
to
achieve
this
goal
using
other
methods
like
a
your
algorithm
may
be
need
to
be
somewhat
either
by
splitting
into
subproblems
that
are
different
in
or
by
performing
different
tasks
on
different
another
you
can
use
the
standard
merging
procedure
with
parts
of
an
array
serving
as
scratch
if
and
denote
four
quarters
of
an
you
can
merge
and
together
into
the
space
occupied
by
and
while
the
elements
of
and
become
scrambled
as
they
are
swapped
back
into
the
space
formerly
occupied
by
and
sorting
algorithm
is
if
it
leaves
equal
elements
in
the
same
relative
order
as
they
were
prior
to
the
is
useful
when
sorting
large
consider
the
email
messages
in
your
each
with
a
a
and
a
subject
you
sort
according
to
and
then
stably
by
then
messages
with
the
same
address
would
remain
sorted
by
insertion
and
merge
sort
are
all
provided
we
implement
them
all
methods
we
know
for
performing
fast
partitioning
seem
inherently
so
stability
seems
hard
to
achieve
without
sacrificing
and
operation
seem
somewhat
at
odds
with
when
it
comes
to
fast
sorting
merge
sort
is
stable
but
not
and
merge
sort
and
heap
sort
in
are
but
not
question
of
whether
stable
and
sorting
is
possible
in
the
model
in
time
remained
open
until
the
late
when
algorithms
fulfilling
all
of
these
requirements
were
finally
to
this
all
such
algorithms
remain
extremely
as
we
see
in
the
next
we
can
sort
stably
and
in
place
in
a
very
simple
and
elegant
fashion
if
we
are
willing
to
settle
for
an
running
and
this
problem
we
construct
variants
of
merge
sort
and
quicksort
that
are
in
and
also
reasonably
are
the
favorite
sorting
algorithms
since
they
provide
a
very
elegant
demonstration
of
the
power
of
the
divide
and
conquer
begin
consider
the
problem
of
swapping
two
adjacent
blocks
in
memory
in
we
have
an
array
in
which
we
identify
a
left
block
and
a
right
block
and
we
would
like
to
rearrange
the
contents
of
the
array
from
to
with
minimal
additional
memory
can
we
do
this
in
that
and
do
not
necessarily
have
the
same
divide
and
design
an
stable
algorithm
for
on
we
obtain
a
stable
version
of
merge
result
from
may
be
of
divide
and
design
an
stable
algorithm
for
on
we
obtain
a
stable
version
of
randomized
result
from
may
again
be
of
the
stable
merging
algorithm
from
part
give
a
simple
alternative
solution
to
problem
a
start
by
dividing
your
array
in
sorting
algorithm
can
be
made
stable
by
using
additional
memory
sacrificing
do
we
augment
each
element
with
its
initial
index
within
the
array
and
modify
our
comparison
operator
to
break
ties
between
equal
elements
using
these
is
a
close
relative
of
the
sorting
asking
us
to
find
the
largest
element
might
say
in
an
unordered
array
element
is
said
to
have
and
is
also
called
the
order
of
the
include
and
although
if
is
there
is
no
unique
and
elements
of
ranks
and
could
both
rightfully
be
called
is
easy
to
solve
in
time
by
first
sorting
our
array
and
then
outputting
the
special
cases
or
it
is
easy
to
perform
selection
in
time
by
simply
scanning
the
array
while
keeping
track
of
the
smallest
element
we
at
other
ranks
is
less
although
we
shall
see
in
this
section
how
to
select
for
the
element
of
any
rank
in
only
median
element
is
a
particularly
useful
since
it
is
commonly
used
as
a
splitting
point
in
algorithms
like
quicksort
to
break
a
large
problem
two
equal
can
solve
the
selection
problem
for
any
in
expected
time
using
a
close
relative
of
randomized
first
partition
about
a
pivot
is
important
to
note
that
partitioning
on
the
median
is
not
an
option
here
with
since
finding
the
median
requires
the
problem
we
are
currently
trying
to
after
which
we
know
the
rank
of
the
since
the
pivot
ends
up
in
its
correct
final
location
if
the
array
were
we
are
lucky
enough
that
the
pivot
is
the
answer
and
we
are
by
comparing
with
we
can
determine
whether
the
element
we
seek
lies
on
the
left
or
right
side
of
the
if
we
recursively
select
for
the
element
of
rank
from
subarray
and
if
we
recursively
select
for
the
element
of
rank
in
the
subarray
is
nearly
identical
in
structure
to
randomized
except
we
recurse
on
only
one
of
the
two
subarrays
created
by
the
partition
not
difference
is
sufficient
to
give
only
a
expected
running
is
one
of
the
few
randomized
algorithms
we
will
study
where
an
expected
running
time
analysis
gives
a
stronger
bound
than
a
high
probability
we
want
a
high
probability
result
for
is
actually
the
best
bound
we
can
claim
it
is
trivial
to
show
since
the
algorithm
is
performing
a
subset
of
the
work
of
randomized
quickselect
in
conjunction
with
a
clever
but
somewhat
complicated
approach
for
choosing
suitable
pivots
we
can
also
solve
the
selection
problem
for
any
in
is
a
nice
result
in
but
since
the
resulting
algorithm
is
rather
complicated
and
also
involves
a
large
hidden
quickselect
is
usually
the
preferred
method
in
to
perform
deterministic
selection
in
linear
of
traditional
selection
problem
involves
computing
one
order
statistic
in
a
instead
that
we
have
a
list
of
order
statistics
that
we
would
like
to
can
clearly
compute
all
of
these
in
time
by
applying
a
selection
algorithm
for
each
one
please
show
how
to
select
for
all
order
statistics
in
only
an
extreme
if
we
set
for
this
boils
down
to
essentially
in
from
wish
to
perform
selection
from
an
array
stored
in
memory
in
an
the
array
elements
is
therefore
as
is
using
more
than
auxiliary
a
randomized
algorithm
for
this
problem
that
runs
in
time
with
high
we
do
not
know
how
to
achieve
a
running
time
with
a
purely
deterministic
we
have
made
the
fundamental
assumption
that
every
pair
of
input
elements
is
this
we
say
our
input
is
a
or
a
total
many
our
input
may
instead
be
a
also
called
a
partial
where
only
certain
pairs
of
input
elements
can
be
meaningfully
result
of
a
comparison
in
a
poset
can
be
or
of
a
directed
acyclic
graph
representation
of
a
and
a
topological
ordering
of
the
nodes
in
this
poset
in
shows
constraints
governing
the
order
in
which
you
can
put
on
different
articles
of
also
shows
how
we
can
represent
a
poset
by
a
directed
acyclic
or
directed
path
in
this
graph
between
two
elements
and
indicates
that
is
less
than
our
this
means
must
precede
when
getting
graph
cannot
have
any
directed
since
if
two
nodes
and
were
on
a
directed
there
would
exist
directed
paths
from
to
and
also
from
to
leaving
it
unclear
which
element
is
the
example
an
edge
from
underwear
to
shoes
is
unnecessary
since
it
is
implied
by
underwear
is
less
than
which
is
in
turn
less
than
we
draw
the
representation
of
a
we
typically
omit
as
many
implied
edges
as
possible
to
simplify
the
with
the
fewest
edges
that
represents
a
given
poset
is
known
as
the
transitive
of
the
and
the
with
all
implied
edges
present
is
called
the
transitive
will
see
how
to
compute
both
of
these
in
can
be
topologically
to
produce
a
total
ordering
of
its
nodes
as
a
topological
that
is
consistent
with
the
partial
ordering
implied
by
the
seen
in
all
edges
point
consistently
from
left
to
right
when
we
arrange
the
according
to
its
topological
topological
orderings
may
be
valid
for
a
given
as
an
extreme
any
ordering
is
valid
if
our
has
no
orderings
have
many
instance
shown
in
can
be
viewed
as
a
where
nodes
represent
tasks
and
edges
represent
precedence
constraints
between
these
arise
commonly
in
this
topological
ordering
here
gives
a
linear
task
ordering
that
respects
all
of
the
precedence
as
sorting
can
be
a
useful
preprocessing
step
for
many
topological
sorting
is
a
common
preprocessing
step
for
algorithms
dealing
with
will
see
several
examples
of
this
when
we
study
graphs
in
greater
detail
later
in
the
are
several
ways
to
topologically
sort
a
in
linear
which
in
this
case
means
time
if
the
has
nodes
and
discuss
one
method
here
and
based
on
a
simple
graph
algorithm
called
in
is
easy
to
show
that
every
always
has
at
least
one
with
no
incoming
you
could
start
at
any
node
and
walk
moving
from
one
node
to
the
next
by
always
stepping
backward
along
an
arbitrary
incoming
the
graph
has
a
finite
number
of
this
process
would
eventually
revisit
a
node
and
close
a
directed
thereby
contradicting
the
fact
that
the
graph
is
source
node
is
safe
to
place
at
the
beginning
of
a
topological
since
no
other
node
needs
to
come
before
we
can
generate
a
valid
topological
ordering
by
repeatedly
finding
and
removing
source
we
can
implement
this
in
linear
is
finally
time
to
depart
from
the
familiar
model
and
consider
algorithms
for
sorting
integers
in
the
range
is
this
allows
us
to
sort
faster
than
by
using
that
we
have
an
array
containing
only
zeros
and
can
easily
sort
in
linear
time
by
simply
counting
the
zeros
and
and
then
building
a
new
sorted
array
containing
the
appropriate
number
of
idea
generalizes
easily
to
the
case
where
contains
integers
in
the
range
we
use
an
auxiliary
array
where
counts
the
number
of
copies
of
the
value
in
build
after
initializing
its
entries
to
we
increment
for
every
can
then
scan
through
to
in
sorted
resulting
sorting
known
as
counting
runs
in
which
is
linear
time
as
long
as
we
can
even
implement
it
in
a
stable
multiple
invocations
of
counting
we
can
build
a
more
powerful
sorting
algorithm
known
as
radix
that
sorts
in
linear
time
if
for
some
constant
operation
of
radix
first
perform
a
stable
sort
on
the
then
on
the
and
so
sort
is
illustrated
in
first
write
our
integers
down
in
some
number
or
the
name
of
the
which
in
our
example
is
base
then
sort
according
to
each
successive
digit
starting
with
the
least
sort
actually
originated
as
mechanical
technique
for
sorting
stacks
of
punched
card
contained
a
number
written
in
binary
encoded
by
a
series
of
punched
and
and
the
cards
were
repeatedly
fed
through
a
mechanical
sorting
once
for
every
and
sorted
by
separating
cards
with
punched
digits
from
those
with
subroutine
we
use
to
perform
each
of
the
digit
sorts
can
be
any
sorting
although
we
use
counting
sort
since
it
is
the
natural
choice
for
sorting
small
we
ideally
write
our
integers
in
base
then
each
digit
assumes
values
from
and
each
individual
counting
sort
runs
in
numbers
will
have
at
most
which
is
constant
as
long
as
for
some
constant
total
running
time
is
therefore
linear
as
long
as
with
common
assumption
with
the
model
of
computation
is
that
each
word
contains
which
is
exactly
large
enough
to
hold
an
integer
of
magnitude
for
the
numbers
being
radix
sorted
each
fit
into
a
single
machine
then
radix
sort
therefore
takes
only
if
we
plan
to
sort
larger
numbers
on
a
machine
with
word
size
it
takes
words
to
store
numbers
of
magnitude
matching
the
running
time
for
radix
sort
this
running
time
cannot
be
since
it
takes
the
same
amount
of
time
just
to
examine
all
the
input
only
way
to
allow
for
stronger
sorting
algorithms
is
to
consider
slightly
larger
word
sizes
is
common
practice
in
the
sorting
word
sizes
slightly
larger
than
it
is
still
unknown
if
sorting
is
possible
see
the
endnotes
for
further
is
crucial
for
our
individual
digit
since
if
two
numbers
agree
in
their
most
significant
digit
final
sorting
we
want
them
to
remain
ordered
properly
by
their
as
they
will
be
at
this
point
thanks
to
sort
can
sort
integers
of
magnitude
for
in
linear
show
how
to
sort
fractions
also
in
linear
where
the
and
are
integers
in
the
range
for
a
can
you
convert
this
problem
back
to
one
involving
sorting
small
radix
sort
works
from
the
least
significant
digit
to
the
most
this
we
do
the
demonstrating
a
variant
called
forward
radix
to
sort
a
collection
of
strings
in
standard
lexicographical
us
suppose
we
have
a
word
size
of
so
radix
sorting
a
list
of
integer
words
would
take
treat
each
string
as
an
array
of
from
the
first
word
down
to
the
we
only
look
the
most
significant
words
of
we
may
not
be
able
to
distinguish
from
some
of
the
other
input
if
they
also
agree
in
these
initial
be
the
minimum
number
of
words
terms
of
a
prefix
of
that
we
need
to
examine
in
order
to
be
able
to
distinguish
from
the
other
input
we
let
then
theoretically
we
should
be
able
to
sort
by
examining
only
total
that
this
framework
also
applies
to
sorting
real
numbers
scaled
to
the
range
specified
as
arrays
of
digits
from
most
significant
down
to
least
significant
here
it
is
crucial
to
use
a
forward
approach
most
significant
digits
rather
than
the
backward
approach
of
standard
radix
since
a
real
number
might
have
an
unbounded
number
of
that
we
typically
do
not
know
at
the
how
to
compute
these
values
in
only
where
we
know
observe
that
we
can
apply
standard
radix
sort
to
sort
in
term
above
is
somewhat
you
improve
the
running
time
for
both
determining
as
well
as
sorting
to
just
running
times
of
counting
sort
and
radix
sort
are
depending
on
the
magnitude
of
our
input
the
running
time
of
counting
sort
depends
directly
on
we
say
counting
sort
has
a
running
whereas
radix
sort
has
a
weakly
polynomial
running
time
since
its
running
time
depends
only
on
to
review
this
of
our
other
sorting
algorithms
have
strongly
polynomial
running
times
only
on
since
they
run
in
the
or
real
models
of
is
an
important
distinction
here
to
be
made
between
these
two
types
of
algorithms
counting
and
radix
versus
or
real
algorithms
merge
sort
and
many
problems
we
it
will
be
possible
to
devise
both
types
of
choice
of
which
is
better
depends
entirely
on
the
neither
is
inherently
sufficiently
small
integer
input
approaches
may
be
algorithms
can
be
perhaps
aesthetically
more
appealing
and
provide
some
peace
of
since
they
do
not
depend
on
the
magnitude
of
their
input
choice
between
and
or
more
generally
between
and
real
algorithms
will
be
a
running
theme
throughout
much
of
this
reinforce
the
techniques
introduced
in
this
we
close
with
a
case
study
showing
how
they
can
be
elegantly
combined
to
solve
a
more
challenging
you
take
tests
in
an
algorithms
earning
on
the
test
points
out
of
maximum
of
possible
is
sometimes
you
are
allowed
to
drop
with
the
final
grade
being
determined
by
a
set
containing
only
the
remaining
goal
is
to
decide
which
tests
to
keep
so
that
our
grade
is
let
denote
this
maximum
is
known
as
the
optimal
subset
selection
and
although
it
may
seem
solvable
by
simply
discarding
the
tests
with
the
lowest
this
is
actually
not
the
may
wish
to
pause
and
convince
yourself
of
this
fact
by
constructing
an
appropriate
on
the
with
any
new
we
might
approach
it
by
attempting
to
apply
one
of
our
main
algorithm
design
and
specifically
in
the
form
of
binary
yields
some
useful
initial
many
optimization
it
turns
out
to
be
much
easier
to
check
whether
a
guess
at
the
answer
is
too
high
or
too
low
than
to
solve
the
problem
time
this
is
the
we
can
home
in
on
the
optimal
solution
value
quickly
using
binary
search
on
check
if
we
want
to
know
if
there
exists
some
subset
of
tests
with
such
that
and
by
rearranging
we
see
that
this
is
the
same
question
as
whether
or
not
there
exists
some
subset
with
such
that
x
we
define
x
we
now
simply
want
to
know
whether
there
is
a
set
of
tests
whose
add
up
to
a
positive
search
on
technique
is
particularly
effective
for
problems
like
this
one
involving
optimization
of
a
since
through
algebraic
term
it
often
reduces
the
problem
to
one
having
a
simpler
given
a
numeric
array
suppose
we
want
to
find
a
window
of
length
at
least
with
the
largest
average
value
the
length
the
problem
would
be
the
answer
being
just
the
single
largest
element
in
test
if
a
guess
for
the
answer
is
too
we
ask
whether
there
exists
a
window
such
that
otherwise
written
as
x
which
we
further
simplify
to
we
now
want
to
know
whether
there
exists
a
subarray
of
of
length
at
least
whose
sum
is
which
we
can
answer
by
finding
the
maximum
sum
of
a
subarray
in
of
length
at
least
a
problem
that
is
easy
to
solve
in
time
have
therefore
effectively
replaced
with
in
our
answer
to
this
question
is
yes
if
and
only
if
the
largest
add
up
to
a
positive
so
we
could
answer
it
by
sorting
the
and
adding
up
the
largest
of
we
can
use
selection
to
obtain
an
answer
in
only
by
selecting
for
the
largest
value
of
and
adding
up
the
values
above
this
us
call
this
algorithm
for
checking
whether
is
too
low
or
too
high
the
optimal
subset
selection
problem
as
the
geometric
problem
of
finding
the
rightmost
value
of
at
which
the
values
of
the
highest
lines
is
still
notation
x
suggests
a
useful
way
to
visualize
the
algorithm
and
the
problem
above
in
general
remember
that
you
can
almost
always
obtain
better
insight
into
solving
a
problem
if
you
can
turn
it
into
a
we
regard
each
test
as
the
line
x
with
slope
and
then
we
can
picture
our
input
as
a
collection
of
lines
as
in
this
geometric
we
want
to
find
the
value
of
at
which
the
sum
of
the
values
of
the
highest
lines
at
evaluated
at
is
our
lines
have
negative
observe
that
the
sum
of
the
values
of
the
highest
lines
decreases
as
we
move
to
the
and
increases
as
we
move
to
the
monotonicity
is
what
makes
binary
search
the
by
the
search
on
to
find
will
work
fine
in
although
this
approach
still
leaves
something
to
be
as
it
only
converges
to
the
optimal
solution
over
rather
than
giving
an
exact
answer
after
a
number
of
steps
polynomial
in
the
real
it
could
theoretically
even
take
infinitely
long
to
converge
if
is
us
therefore
consider
a
slightly
different
we
magically
knew
the
highest
lines
when
evaluated
at
that
this
is
enough
to
compute
in
since
we
can
add
together
the
linear
functions
representing
these
lines
to
get
a
single
linear
function
of
set
it
equal
to
and
solve
for
all
we
need
to
do
is
compute
the
highest
lines
at
we
would
do
this
in
time
by
using
quickselect
to
select
for
the
highest
line
at
and
then
partitioning
on
this
line
to
obtain
the
lines
above
may
all
seem
since
we
know
so
we
know
what
value
of
to
plug
in
when
evaluating
our
let
us
keep
the
in
the
form
of
generic
linear
functions
while
running
the
algorithm
cause
problems
until
we
reach
a
where
we
are
now
comparing
two
linear
such
as
versus
result
of
this
comparison
depends
on
the
first
function
is
smaller
if
and
larger
if
that
we
are
planning
to
evaluate
these
functions
at
all
we
need
to
know
to
resolve
the
comparison
is
therefore
whether
or
not
and
we
can
answer
this
question
by
running
approach
now
makes
we
run
our
selection
algorithm
with
the
represented
generically
as
linear
we
pause
at
every
comparison
and
run
an
invocation
of
to
test
the
value
of
that
is
necessary
to
resolve
the
every
comparison
in
the
algorithm
invokes
a
the
total
running
time
is
of
and
final
idea
gives
a
dramatic
improvement
in
running
algorithm
based
on
quickselect
performs
a
number
of
partition
each
one
comparing
all
the
elements
in
an
array
against
a
specific
pivot
these
comparisons
depend
on
could
in
theory
be
done
in
we
can
therefore
use
divide
and
conquer
based
on
selection
and
binary
search
to
resolve
such
comparisons
in
only
time
plus
only
invocations
of
of
invocations
as
drops
our
total
running
time
to
only
in
technique
known
as
parametric
is
useful
in
a
wide
range
of
particularly
in
computational
is
also
an
interesting
example
of
how
ideas
from
parallel
computation
can
be
used
in
novel
ways
to
speed
up
sequential
our
it
serves
as
a
wonderful
example
of
what
one
can
achieve
using
smart
combinations
of
basic
design
the
endnotes
for
further
references
regarding
the
optimal
subset
selection
as
this
problem
even
admits
a
and
ideal
sorting
algorithm
would
be
run
in
time
for
and
require
only
memory
far
we
only
how
to
achieve
certain
specific
combinations
of
these
so
despite
the
fact
that
sorting
is
such
a
fundamental
it
continues
to
challenge
even
the
best
computing
is
a
key
step
in
many
algorithms
algorithms
in
computational
and
we
will
see
problems
involving
sorting
throughout
this
we
study
data
structures
in
the
next
few
we
will
learn
even
more
ways
to
using
data
structures
like
priority
queues
and
binary
search
as
a
of
common
use
of
sorting
during
preprocessing
is
simply
to
group
similar
elements
a
nice
example
of
this
approach
in
please
describe
an
algorithm
that
takes
points
in
the
plane
as
input
and
counts
the
number
of
parallelograms
whose
corners
all
come
from
the
point
you
count
rectangles
as
well
necessarily
aligned
with
the
and
and
problem
contains
a
collection
of
nice
practice
problems
that
can
be
solved
elegantly
using
the
divide
and
conquer
we
want
to
compute
where
is
an
could
do
this
by
starting
with
and
successively
multiplying
by
for
since
matrix
multiplication
is
a
operation
straightforward
approach
for
multiplying
two
matrices
gives
has
an
running
we
would
like
to
use
as
few
multiplications
as
show
how
to
compute
using
only
matrix
multiplications
and
additions
that
matrix
additions
are
much
less
costly
these
only
take
a
remember
that
the
technique
you
are
developing
goes
by
the
name
of
repeated
its
this
method
is
quite
and
it
is
commonly
used
in
practice
for
raising
any
complicated
object
a
matrix
or
a
or
any
other
object
for
which
multiplication
is
to
a
large
are
given
two
sorted
arrays
and
as
show
how
to
compute
the
median
among
all
the
elements
contained
in
both
arrays
in
a
what
is
the
best
performance
guarantee
you
can
offer
terms
of
and
if
and
contain
and
elements
rather
than
elements
if
you
want
to
find
the
median
element
among
sorted
arrays
each
containing
the
are
given
two
containing
and
containing
of
the
elements
present
in
a
algorithm
running
in
time
that
determines
which
element
in
does
not
appear
in
quickly
can
you
solve
the
more
general
problem
variant
where
elements
are
left
out
of
about
the
similar
problem
where
contains
elements
including
all
of
the
elements
in
but
with
one
of
them
occurring
twice
where
we
wish
to
locate
the
duplicated
of
an
is
obtained
by
performing
a
right
cyclic
shift
of
a
array
whose
elements
form
a
strictly
increasing
sequence
right
cyclic
shift
moves
every
element
to
an
index
steps
with
the
final
elements
and
becoming
the
first
how
can
you
determine
in
be
an
matrix
whose
entries
are
distinct
and
let
denote
index
of
the
minimum
element
in
row
say
is
if
give
an
algorithm
for
computing
in
an
monotone
we
will
see
monotone
matrices
with
even
more
known
as
totally
for
which
we
can
actually
compute
in
only
time
the
case
of
a
monotone
matrix
there
is
actually
an
lower
bound
in
the
comparison
we
will
this
result
has
surprisingly
many
you
want
to
find
an
unknown
value
in
the
range
is
an
we
can
of
course
binary
search
for
in
time
by
first
guessing
then
seeing
if
this
guess
is
too
high
or
too
and
successively
halving
our
search
range
in
each
that
is
not
necessarily
an
and
that
we
are
content
to
find
an
approximate
solution
within
the
range
where
is
some
it
turns
we
can
solve
this
problem
in
only
time
by
using
the
geometric
mean
rather
than
the
arithmetic
mean
as
our
guess
for
each
step
in
the
binary
when
we
are
searching
the
interval
our
next
guess
will
be
prove
that
this
does
indeed
give
us
an
running
a
consider
problem
of
are
given
an
array
containing
distinct
numbers
that
describe
the
heights
of
people
standing
in
a
say
person
and
person
can
see
if
and
are
both
larger
than
all
of
the
elements
in
show
how
to
locate
a
farthest
pair
of
individuals
and
that
can
still
see
in
problem
we
improve
the
running
time
to
a
are
given
as
input
a
pointer
to
the
first
element
of
an
linked
list
being
told
and
asked
to
print
out
the
contents
of
the
list
in
reverse
order
without
physically
modifying
the
elements
of
the
list
in
any
is
trivial
to
do
in
time
if
we
allow
extra
but
somewhat
more
interesting
if
we
wish
to
use
much
less
auxiliary
memory
while
still
maintaining
a
fast
running
describe
an
time
solution
using
only
auxiliary
words
of
you
generalize
your
solution
to
run
in
time
using
space
for
any
that
gives
the
trivial
solution
where
we
use
extra
and
that
leads
to
the
time
and
space
bounds
of
and
a
be
a
function
defined
over
all
integers
in
the
range
say
is
a
local
minimum
of
if
and
where
we
treat
show
how
to
compute
a
local
minimum
of
by
evaluating
at
only
let
be
a
function
defined
over
integers
and
in
the
range
say
is
a
local
minimum
if
is
no
larger
than
any
of
the
four
neighboring
values
and
y
we
treat
as
being
infinite
if
or
is
not
in
the
range
show
how
to
compute
a
local
minimum
of
with
only
function
operation
counts
the
number
of
bits
set
to
one
in
the
binary
representation
of
a
nonnegative
integer
plays
a
useful
role
in
surprisingly
many
is
a
can
easily
be
computed
in
time
by
summing
up
the
individual
bits
in
show
how
to
use
divide
and
conquer
to
obtain
an
running
assume
the
following
machine
instructions
are
available
and
take
constant
time
shifting
the
bits
of
left
or
right
any
number
of
and
bitwise
and
the
bits
of
left
by
positions
corresponds
to
integer
multiplication
by
and
bitwise
operations
on
two
integers
and
perform
or
on
each
corresponding
pair
of
bits
in
and
in
we
can
isolate
the
value
of
the
bit
in
by
shifting
left
by
bitwise
the
result
with
mask
out
just
the
bit
we
care
and
then
shifting
right
by
you
want
to
divide
up
a
cake
among
people
so
that
every
person
feels
like
he
or
she
has
received
a
is
easy
if
everyone
values
every
part
of
the
cake
since
the
solution
in
this
case
is
just
to
give
of
the
cake
to
in
practice
we
might
find
that
different
individuals
might
have
preferences
over
different
parts
of
the
cake
might
like
the
side
of
the
cake
with
more
and
you
might
prefer
the
side
of
the
cake
with
more
sprinkles
on
be
somewhat
more
let
us
consider
the
unit
interval
as
a
long
of
our
participants
must
be
served
a
piece
of
cake
that
corresponds
to
a
contiguous
piece
of
this
preferences
of
participant
are
described
by
a
valuation
function
that
specifies
value
for
the
interval
function
is
monotonically
and
ranges
from
up
to
the
whole
cake
is
worth
unit
to
each
that
we
can
find
the
value
of
any
subinterval
by
taking
addition
to
suppose
we
also
have
access
to
its
inverse
function
so
for
example
would
tell
us
the
unique
point
such
that
exactly
half
of
the
value
for
participant
lies
in
and
the
other
half
lies
in
that
it
takes
time
to
evaluate
or
an
time
algorithm
that
computes
a
subinterval
for
each
participant
such
that
each
participant
receives
what
he
or
she
perceives
to
be
a
piece
of
the
of
are
given
points
where
the
and
are
integers
in
the
range
points
and
are
said
to
be
if
and
and
as
please
describe
an
algorithm
for
counting
the
number
of
pairs
of
a
you
may
want
to
consider
is
a
classical
problem
in
parallel
algorithm
design
that
has
an
elegant
solution
based
on
the
principle
of
divide
and
we
have
processors
connected
in
a
processor
is
synchronized
with
a
global
and
each
time
the
clock
ticks
the
processor
can
perform
work
as
well
as
send
information
to
its
neighbors
it
takes
time
steps
for
a
message
from
one
end
of
the
line
to
propagate
down
to
the
other
each
processor
is
somewhat
limited
in
that
it
only
has
a
constant
number
of
bits
of
independent
of
prohibits
processors
from
doing
things
like
counting
to
would
require
bits
of
goal
is
to
design
an
algorithm
to
run
continuously
on
these
processors
processor
should
run
the
same
such
that
if
we
input
a
special
message
to
one
of
the
endpoint
processors
at
some
all
processors
should
simultaneously
enter
a
special
state
they
should
all
at
some
point
time
steps
you
devise
a
simple
algorithm
that
accomplishes
this
can
easily
solve
the
problem
of
finding
the
element
in
an
array
in
time
by
and
we
also
have
an
lower
bound
on
this
problem
in
the
comparison
model
via
a
simple
reduction
from
the
element
uniqueness
if
the
element
appears
more
than
times
in
the
we
can
solve
this
special
case
in
only
time
do
we
do
motivates
us
to
try
and
design
an
algorithm
for
finding
the
element
whose
running
time
scales
gracefully
between
and
depending
on
the
number
of
occurrences
of
the
show
how
to
solve
this
problem
in
and
if
you
are
feeling
try
to
prove
a
matching
lower
bound
in
the
the
problem
of
merging
lists
containing
total
are
several
ways
to
solve
this
problem
in
one
can
perform
the
standard
iterative
merging
algorithm
using
a
data
structure
such
as
a
binary
heap
or
balanced
binary
search
tree
to
maintain
the
leading
elements
in
our
in
every
we
select
the
minimum
such
element
in
adding
it
next
to
the
merged
comment
on
how
we
might
instead
solve
this
problem
using
a
argue
that
there
is
a
matching
lower
bound
of
in
the
model
for
any
algorithm
that
performs
merging
of
from
a
we
have
array
of
elements
drawn
from
a
universe
of
only
distinct
and
we
wish
to
sort
this
array
using
a
sorting
is
small
we
would
hope
to
improve
on
the
running
time
for
generic
sorting
you
modify
merge
sort
to
solve
this
sorting
problem
in
only
algorithm
will
not
be
told
the
value
of
that
randomized
quicksort
applied
to
this
problem
runs
in
time
with
high
even
though
it
also
does
not
know
the
value
of
that
there
is
a
lower
bound
of
on
the
running
time
of
any
algorithm
that
solves
this
even
if
the
algorithm
knows
the
distinct
values
appearing
throughout
the
you
are
given
two
strings
and
whose
characters
are
drawn
from
an
the
letters
through
wish
to
detect
whether
or
not
and
are
that
whether
or
not
you
can
transform
into
by
permuting
its
show
that
in
the
model
of
computation
one
can
detect
anagrams
in
and
also
show
that
there
is
a
matching
lower
bound
of
on
the
running
time
of
any
anagram
detection
that
another
nice
solution
for
sorting
in
time
will
become
apparent
once
we
learn
in
to
use
balanced
binary
search
trees
as
in
that
we
insert
our
values
in
to
a
balanced
binary
search
tree
where
each
element
is
augmented
with
a
frequency
count
takes
since
the
tree
contains
and
then
we
traverse
the
tree
to
enumerate
its
contents
in
sorted
sorting
is
very
useful
for
proving
correctness
of
complicated
sorting
states
that
a
sorting
algorithm
is
correct
if
and
only
if
it
correctly
sorts
sequences
consisting
of
just
zeros
and
in
order
to
argue
correctness
of
a
complicated
sorting
it
suffices
to
argue
that
it
sorts
any
sequence
of
zeros
and
give
a
short
proof
of
the
sorting
and
us
think
of
an
array
as
interleaved
of
first
chain
is
the
second
is
and
so
say
an
array
is
if
each
of
its
chains
is
can
also
think
of
an
array
in
terms
of
of
size
first
block
is
the
second
is
and
so
say
is
if
each
of
its
blocks
is
is
the
fastest
possible
running
time
the
comparison
for
making
an
array
or
is
the
fastest
possible
running
time
in
the
comparison
for
sorting
an
array
that
is
already
and
an
array
that
is
and
notational
let
and
let
we
want
to
search
for
a
particular
element
based
on
its
algorithms
that
perform
this
task
in
time
time
more
and
time
bit
more
that
the
last
running
time
dominates
both
of
the
other
array
that
is
both
and
for
can
be
visualized
as
the
contents
of
a
matrix
of
elements
whose
rows
and
columns
are
all
the
preceding
we
showed
how
to
search
in
such
a
structure
in
quickly
can
you
search
in
the
generalization
of
this
if
then
your
data
would
be
stored
in
a
array
of
size
that
is
sorted
along
each
that
if
we
sorting
each
of
its
blocks
an
array
that
is
already
the
array
remains
and
also
that
for
any
and
if
we
sort
sorting
each
of
its
chains
an
array
that
is
already
then
the
array
stays
the
same
true
for
and
named
after
its
creator
is
a
method
for
speeding
up
insertion
sort
by
allowing
elements
to
be
moved
longer
distances
the
context
of
the
preceding
it
involves
making
an
array
calling
insertion
sort
on
each
of
its
chains
for
decreasing
values
of
coming
from
a
increment
suppose
we
use
the
increment
sequence
of
integers
one
less
than
powers
of
so
we
start
with
being
the
largest
increment
in
this
sequence
less
than
and
end
with
sequences
always
end
with
since
this
ensures
the
final
array
is
sorted
that
means
the
same
thing
as
sort
runs
in
time
for
the
increment
sequence
but
a
better
sequence
least
in
although
sometimes
not
in
is
the
set
containing
all
integers
of
the
form
of
which
there
are
elements
at
most
the
result
from
the
last
part
of
the
preceding
we
know
that
when
it
comes
time
to
our
it
will
already
be
and
that
this
implies
that
for
each
increment
takes
only
leading
to
an
overall
running
time
of
which
is
nearly
the
best
attainable
for
sort
the
endnotes
for
further
a
remember
that
insertion
sort
runs
quickly
when
there
are
few
inversions
in
the
arrays
being
in
a
on
your
solution
to
problem
please
now
consider
the
problem
of
selecting
the
largest
element
from
an
matrix
whose
rows
and
columns
are
both
a
see
if
you
can
solve
this
problem
in
only
assume
for
simplicity
that
all
entries
in
the
matrix
are
a
try
partitioning
the
matrix
into
small
blocks
or
in
and
construct
a
smaller
matrix
by
replacing
each
block
with
its
minimum
or
by
recursively
solving
a
selection
problem
in
this
smaller
you
should
be
able
to
exclude
from
consideration
many
of
the
elements
in
the
original
of
a
a
sorting
network
that
sorts
and
an
sorting
network
that
implements
merge
blocks
labeled
recursively
sort
inputs
of
length
and
the
block
labeled
merges
their
output
into
a
sorted
list
of
length
this
problem
we
briefly
delve
into
the
realm
of
parallel
sorting
common
model
for
parallel
sorting
is
the
sorting
a
circuit
taking
numbers
as
input
that
produces
their
sorted
ordering
as
example
of
a
sorting
network
is
shown
in
networks
are
built
from
blocks
called
that
compare
two
input
numbers
and
output
the
numbers
in
sorted
sorting
network
is
built
from
a
series
of
each
of
which
contains
several
each
stage
can
be
at
most
one
comparator
a
signal
may
not
pass
through
two
different
comparators
in
the
same
goal
of
a
sorting
network
is
to
minimize
the
number
of
as
this
corresponds
to
the
running
time
of
the
it
does
not
seem
possible
to
map
any
of
our
most
common
sorting
algorithms
merge
sort
and
to
an
efficient
parallel
sorting
there
are
several
elegant
approaches
for
constructing
efficient
sorting
discuss
two
of
these
we
wish
to
merge
two
sorted
arrays
and
into
a
single
sorted
array
the
following
somewhat
strange
recursive
approach
for
performing
this
the
odd
elements
of
are
obtained
by
recursively
merging
the
odd
elements
of
and
and
the
even
elements
of
are
obtained
by
recursively
merging
the
even
elements
of
and
the
even
elements
of
running
these
recursive
merges
and
filling
up
we
make
one
final
postprocessing
pass
in
which
we
compare
successive
pairs
of
elements
in
versus
versus
and
so
swapping
any
pairs
that
are
out
of
use
the
sorting
theorem
to
prove
that
this
algorithm
merges
show
how
to
build
a
sorting
network
based
on
this
merge
operation
that
requires
total
way
to
merge
two
sorted
sequences
is
to
concatenate
the
first
with
the
reversal
of
the
second
and
then
to
sort
the
resulting
call
a
sequence
bitonic
if
it
increases
up
until
some
point
and
then
or
if
it
is
a
of
such
a
it
may
seem
like
moving
in
the
wrong
direction
by
converting
a
merging
problem
back
into
a
sorting
it
turns
out
that
bitonic
sorting
is
a
natural
and
easy
problem
on
a
sorting
contains
a
bitonic
a
single
stage
of
our
sorting
we
place
comparators
between
and
and
and
so
executing
this
show
the
sorting
that
the
elements
in
must
all
be
no
larger
than
the
elements
in
and
that
these
two
length
must
also
be
a
we
can
now
continue
to
sort
these
two
subproblems
recursively
in
that
merging
according
to
this
strategy
requires
and
that
this
leads
to
a
parallel
version
of
merge
sort
that
requires
total
know
the
locations
of
businesses
in
the
downtown
section
of
our
city
these
locations
are
simply
coordinates
in
the
the
streets
in
downtown
are
shaped
like
a
we
measure
distances
using
the
or
in
which
the
distance
between
two
points
and
is
given
by
we
wish
to
choose
the
coordinates
of
a
post
office
such
so
as
to
minimize
the
sum
of
distances
between
the
post
office
and
each
how
to
find
a
suitable
location
for
the
post
office
in
you
want
to
start
with
a
simpler
consider
first
the
variant
where
the
businesses
are
points
on
a
number
you
generalize
your
solution
to
work
in
higher
us
now
assign
weights
to
the
indicating
the
relative
important
of
the
post
office
being
close
to
each
now
wish
to
find
a
location
for
the
post
office
which
minimizes
the
weighted
sum
of
distances
to
the
how
to
solve
this
problem
in
this
problem
we
will
attempt
to
count
the
inversions
in
an
will
see
additional
solutions
to
this
problem
in
problems
and
the
merge
sort
algorithm
to
count
the
total
number
of
inversions
in
an
array
while
still
running
in
to
extend
this
algorithm
to
count
the
number
of
inversions
per
array
define
the
number
of
inversions
for
an
element
as
the
number
of
elements
preceding
it
in
the
array
that
have
larger
values
than
only
consider
preceding
elements
so
we
count
each
inversion
exactly
once
summing
up
all
of
the
inversion
counts
should
therefore
give
the
total
number
of
we
would
like
to
efficiently
count
inversions
in
an
array
of
integers
in
the
range
first
that
if
then
we
can
do
this
in
linear
for
then
we
can
sort
in
linear
time
via
radix
sort
but
it
seems
difficult
to
exactly
count
inversions
in
linear
we
can
approximate
the
number
of
inversions
in
linear
summing
up
for
each
element
in
our
array
the
distance
between
its
current
index
in
the
array
and
its
rank
its
index
within
the
array
once
the
array
is
resulting
is
known
as
that
we
can
compute
this
quantity
in
linear
and
that
where
denotes
the
number
of
inversions
in
our
are
given
as
input
an
array
of
real
numbers
and
a
target
value
describe
how
to
count
the
number
of
subarrays
with
at
least
in
describe
how
to
count
the
number
of
subarrays
with
at
least
in
or
better
time
a
subarray
of
even
this
means
that
at
least
half
its
elements
must
be
no
smaller
than
describe
how
to
count
the
number
of
subarrays
with
at
least
in
are
many
situations
in
practice
where
we
might
want
to
solve
a
rank
problem
that
involves
finding
an
ordering
of
elements
that
is
to
a
set
of
different
in
an
athletic
competition
with
we
may
have
judges
who
each
independently
rank
the
after
which
we
need
to
generate
a
single
ranking
that
agrees
the
greatest
extent
with
the
rankings
of
all
when
conducting
a
web
it
may
be
more
robust
to
somehow
combine
the
rankings
of
pages
by
different
search
rather
than
to
use
the
rankings
returned
by
only
a
single
search
given
ranked
lists
on
the
same
set
of
that
we
wish
to
construct
a
single
ranked
list
minimizing
where
denotes
the
number
of
inversions
between
lists
and
problem
is
although
it
can
be
approximated
well
also
the
endnotes
for
more
please
show
that
if
we
choose
one
of
the
input
orderings
uniformly
at
then
this
gives
a
fact
implies
that
if
we
take
the
best
of
the
we
also
obtain
a
since
the
minimum
of
a
set
of
numbers
is
always
no
larger
than
the
can
either
solve
this
problem
directly
if
you
using
the
result
of
problem
the
second
all
you
need
to
show
is
that
the
inversion
distance
between
two
orderings
is
a
for
any
three
orderings
and
the
triangle
inequality
discuss
an
alternate
solution
based
on
network
flow
algorithms
in
problem
are
given
intervals
on
the
number
assume
all
interval
endpoints
are
distinct
assumption
is
not
fundamentally
intervals
can
be
related
in
possible
they
can
interval
within
the
they
can
be
overlapping
at
or
they
can
but
not
set
of
intervals
is
said
to
be
if
there
are
no
crossing
pairs
of
that
if
two
intervals
overlap
at
they
must
intervals
exhibit
a
sort
of
an
algorithm
for
testing
whether
or
not
a
set
of
intervals
is
a
see
if
you
can
prove
an
lower
bound
on
the
running
time
of
any
deterministic
algorithm
that
solves
part
for
each
interval
in
our
set
we
wish
to
compute
its
containment
number
of
intervals
contained
within
the
specified
its
inclusion
number
of
other
intervals
containing
the
specified
as
well
as
its
overlap
number
of
other
intervals
overlapping
the
specified
not
counting
those
containing
or
contained
within
the
how
to
compute
these
quantities
for
each
of
the
input
intervals
in
we
have
a
collection
of
sets
each
containing
some
subset
of
the
elements
each
we
are
given
as
input
a
list
of
the
elements
it
sets
and
are
said
to
be
if
to
if
or
and
to
every
pair
of
sets
is
disjoint
or
we
say
our
collection
of
sets
is
design
an
algorithm
for
checking
whether
a
collection
of
sets
is
you
improve
the
running
time
to
n
where
is
the
total
number
of
elements
in
all
the
input
and
on
a
this
problem
we
investigate
the
computation
of
statistical
information
about
the
set
of
distances
between
points
on
a
number
you
are
given
the
locations
of
these
points
necessarily
an
algorithm
for
computing
the
average
distance
among
all
pairs
of
a
randomized
algorithm
running
in
time
with
high
probability
that
computes
the
median
of
all
pairwise
a
can
you
find
a
deterministic
algorithm
for
the
problem
from
part
with
reversal
between
sequences
and
is
the
minimum
number
of
substring
reversals
required
to
transform
into
we
consider
to
be
the
sorted
order
of
asking
us
to
sort
with
a
minimum
number
of
substring
problem
of
computing
reversal
distance
sorting
by
has
application
in
computational
where
the
reversal
distance
between
two
strands
of
gives
some
indication
of
their
evolutionary
is
general
problem
of
computing
the
reversal
distance
between
two
strings
is
known
to
be
although
somewhat
surprisingly
its
variant
can
be
solved
in
polynomial
time
by
a
rather
complicated
algorithm
references
can
be
found
in
the
signed
variant
is
particularly
relevant
to
some
biological
situations
where
elements
of
our
sequence
each
have
an
inherent
notion
of
for
by
reversing
the
substring
in
we
would
end
up
with
where
denotes
the
element
oriented
in
the
reverse
sequence
of
reversals
in
the
signed
case
must
leave
every
element
oriented
in
the
forward
this
we
consider
only
the
even
in
the
special
case
where
we
are
only
allowed
to
reverse
prefixes
of
our
affectionately
known
as
the
pancake
problem
since
it
models
the
situation
where
we
are
trying
to
sort
a
stack
of
pancakes
of
different
sizes
using
a
spatula
with
which
we
can
only
flip
over
a
group
of
pancakes
at
the
top
of
the
signed
variant
is
known
as
the
where
each
pancake
has
a
burned
and
all
of
these
need
to
end
up
facing
simple
approach
for
pancake
flipping
is
to
place
the
largest
pancake
at
the
bottom
of
the
stack
it
already
by
first
flipping
it
up
to
the
and
the
reversing
the
entire
then
proceed
to
place
the
and
so
only
moving
pancakes
that
need
to
be
argue
that
this
is
actually
a
a
a
useful
concept
here
is
the
notion
of
a
which
exists
between
two
pancakes
if
they
are
not
neighbors
in
the
final
sorted
consider
the
general
problem
of
sorting
with
substring
natural
greedy
algorithm
for
this
problem
is
to
a
substring
that
reduces
the
number
of
breakpoints
in
our
sequence
as
much
as
a
reversal
can
only
affect
breakpoints
at
its
the
best
we
can
hope
to
do
is
reduce
the
number
of
breakpoints
by
worst
we
can
do
is
fail
to
reduce
the
number
of
breakpoints
at
all
we
need
to
be
careful
in
proving
that
the
algorithm
actually
a
if
the
algorithm
can
only
manage
to
decrease
the
number
of
breakpoints
by
then
it
should
make
its
choice
in
such
a
way
that
it
leaves
a
decreasing
if
strip
is
just
a
substring
of
elements
between
any
two
consecutive
which
can
be
either
increasing
or
a
prove
that
the
greedy
algorithm
is
a
a
starting
show
that
the
greedy
algorithm
uses
only
reversals
to
sort
a
sequence
with
as
long
as
it
has
at
least
one
decreasing
strip
if
there
are
not
any
decreasing
strips
our
first
reversal
will
create
so
we
will
use
at
most
reversals
in
and
playing
a
perfect
involves
taking
the
top
and
bottom
halves
of
a
deck
of
cards
and
interleaving
the
we
think
of
the
deck
of
cards
as
an
array
of
length
having
elements
followed
by
then
after
a
perfect
shuffle
the
array
would
contain
we
design
elegant
algorithms
for
perfect
shuffles
and
related
problems
that
operate
in
algorithms
for
performing
a
perfect
shuffle
and
the
inverse
of
a
perfect
a
consider
problem
we
store
an
matrix
in
form
as
an
array
of
length
in
which
we
list
the
elements
of
the
matrix
one
row
at
a
time
from
left
to
taking
the
of
such
a
we
convert
it
into
form
an
array
of
length
listing
the
columns
of
the
matrix
one
at
a
each
one
from
top
to
perfect
shuffle
is
equivalent
to
computing
the
transpose
of
a
and
more
generally
the
transpose
of
an
matrix
can
be
seen
as
an
perfect
where
we
divide
a
sequence
into
equal
blocks
and
then
interleave
their
the
result
from
as
black
show
how
to
compute
the
transpose
of
an
arbitrary
matrix
in
place
in
a
to
the
previous
we
now
develop
a
simple
algorithm
for
performing
an
arbitrary
as
long
as
we
also
know
the
inverse
of
the
permutation
is
true
for
the
perfect
shuffle
and
matrix
transposition
permutations
permuting
an
array
according
to
some
permutation
where
specifies
the
new
index
into
which
the
element
should
be
are
also
given
the
inverse
of
our
permutation
so
and
apply
our
permutation
by
shifting
elements
around
its
we
move
to
index
which
displaces
element
which
we
then
move
to
index
and
so
until
we
close
a
cycle
and
place
an
element
back
at
index
can
trace
out
such
a
cycle
starting
from
any
element
in
our
and
if
that
element
happens
to
have
the
smallest
index
among
all
elements
in
the
we
call
that
element
the
of
the
algorithm
scans
through
all
array
indices
and
rotates
the
cycle
containing
element
only
if
is
the
cycle
thereby
ensuring
each
cycle
is
rotated
only
we
would
ensure
each
cycle
is
rotated
only
once
by
simply
marking
its
elements
when
they
are
that
give
an
so
we
use
the
leader
trick
only
remaining
problem
is
to
determine
if
index
is
the
leader
of
its
do
we
could
scan
forward
to
and
so
until
we
either
encounter
an
index
smaller
than
which
case
was
not
the
or
we
return
to
which
case
was
the
may
take
too
long
if
our
permutation
has
long
so
instead
we
walk
outward
simultaneously
in
visiting
and
then
and
and
so
until
we
again
reach
an
index
less
than
or
return
to
to
prove
that
regardless
of
our
this
bidirectional
scan
results
in
an
total
running
time
for
our
permutation
may
want
to
also
consider
how
this
analysis
leads
to
an
time
solution
to
problem
as
via
intersting
result
related
to
the
perfect
shuffle
question
above
involves
the
riffle
a
where
we
cut
a
sequence
at
some
point
of
our
choosing
and
then
arbitrarily
interleave
the
resulting
pieces
and
both
and
are
subsequences
of
the
on
your
experience
with
divide
and
please
argue
that
riffle
shuffles
are
sufficient
to
enact
permutation
of
